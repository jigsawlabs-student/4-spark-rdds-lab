{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complimentary-burns",
   "metadata": {},
   "source": [
    "# Spark RDDs Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-congress",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-pledge",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice working with Spark RDDs and the Spark UI.  Along the way, we'll explore the different attributes about RDDs:\n",
    "    \n",
    "* in memory storage\n",
    "* distributed jobs performed in parallel \n",
    "* resiliency through the use of dags\n",
    "* and lazy operations that only kick off when an action is invoked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-decision",
   "metadata": {},
   "source": [
    "Ok, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-subscription",
   "metadata": {},
   "source": [
    "### Getting Setup (On Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-moral",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-comment",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-battle",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"netflix\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-label",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "explicit-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-ballot",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-forest",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-shooting",
   "metadata": {},
   "source": [
    "Let's begin by creating our spark context.  Set the number of partitions equal to 2, and pass through the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mysterious-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"s3://jigsaw-labs/spotify_tracks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lined-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-account",
   "metadata": {},
   "source": [
    "So if we look at one of the records, we can see that we have a list of song tracks with the artist name, a track id, and various attributes of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecological-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-player",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acousticness        float64\n",
       "artists              object\n",
       "danceability        float64\n",
       "duration_ms           int64\n",
       "energy              float64\n",
       "explicit              int64\n",
       "id                   object\n",
       "instrumentalness    float64\n",
       "key                   int64\n",
       "liveness            float64\n",
       "loudness            float64\n",
       "mode                  int64\n",
       "name                 object\n",
       "popularity            int64\n",
       "release_date         object\n",
       "speechiness         float64\n",
       "tempo               float64\n",
       "valence             float64\n",
       "year                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-treasury",
   "metadata": {},
   "source": [
    "### Creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-style",
   "metadata": {},
   "source": [
    "Ok, next let's create an RDD from our music tracks below by feeding it into the parallelize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "precious-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_rdd = None\n",
    "\n",
    "tracks_rdd\n",
    "# ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-hawaii",
   "metadata": {},
   "source": [
    "Now if we look at the Spark UI, note that there are no jobs listed, and the event timeline should be blank.  So this goes back to Spark being lazy.  Even though we directed Spark to read in the data, Spark will not take action until we invoke an action.  \n",
    "\n",
    "> In fact, if we look at the executors page of the dashboard, we'll see that none of our memory was consumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-crossing",
   "metadata": {},
   "source": [
    "<img src=\"./executors.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-emphasis",
   "metadata": {},
   "source": [
    "Ok, so now let's call our first action by asking for the number of records in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "classified-jefferson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174389"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tracks\n",
    "\n",
    "\n",
    "# 174389"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-cliff",
   "metadata": {},
   "source": [
    "And then let's call another action by looking at the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ordered-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acousticness': 0.991,\n",
       "  'artists': \"['Mamie Smith']\",\n",
       "  'danceability': 0.598,\n",
       "  'duration_ms': 168333,\n",
       "  'energy': 0.2239999999999999,\n",
       "  'explicit': 0,\n",
       "  'id': '0cS0A1fUEUd1EW3FcF8AEI',\n",
       "  'instrumentalness': 0.000522,\n",
       "  'key': 5,\n",
       "  'liveness': 0.379,\n",
       "  'loudness': -12.628,\n",
       "  'mode': 0,\n",
       "  'name': 'Keep A Song In Your Soul',\n",
       "  'popularity': 12,\n",
       "  'release_date': '1920',\n",
       "  'speechiness': 0.0936,\n",
       "  'tempo': 149.976,\n",
       "  'valence': 0.634,\n",
       "  'year': 1920}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [{'acousticness': 0.991,\n",
    "#   'artists': \"['Mamie Smith']\",\n",
    "#   'danceability': 0.598,\n",
    "#   'duration_ms': 168333,\n",
    "#   'energy': 0.2239999999999999,\n",
    "#   'explicit': 0,\n",
    "#   'id': '0cS0A1fUEUd1EW3FcF8AEI',\n",
    "#   'instrumentalness': 0.000522,\n",
    "#   'key': 5,\n",
    "#   'liveness': 0.379,\n",
    "#   'loudness': -12.628,\n",
    "#   'mode': 0,\n",
    "#   'name': 'Keep A Song In Your Soul',\n",
    "#   'popularity': 12,\n",
    "#   'release_date': '1920',\n",
    "#   'speechiness': 0.0936,\n",
    "#   'tempo': 149.976,\n",
    "#   'valence': 0.634,\n",
    "#   'year': 1920}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-election",
   "metadata": {},
   "source": [
    "### Querying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-luxury",
   "metadata": {},
   "source": [
    "From here, we can perform some queries on the data.  Let's begin by getting a sense of the range of our data.  Sort the tracks by year and then let's look at the first record.\n",
    "\n",
    "> Take a look at the [Pyspark documentation](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#sortBy) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "elect-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTracks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "prospective-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTracks.take(1)\n",
    "\n",
    "# [{'acousticness': 0.991,\n",
    "#   'artists': \"['Mamie Smith']\",\n",
    "#   'danceability': 0.598,\n",
    "#   'duration_ms': 168333,\n",
    "#   'energy': 0.2239999999999999,\n",
    "#   'explicit': 0,\n",
    "#   'id': '0cS0A1fUEUd1EW3FcF8AEI',\n",
    "#   'instrumentalness': 0.000522,\n",
    "#   'key': 5,\n",
    "#   'liveness': 0.379,\n",
    "#   'loudness': -12.628,\n",
    "#   'mode': 0,\n",
    "#   'name': 'Keep A Song In Your Soul',\n",
    "#   'popularity': 12,\n",
    "#   'release_date': '1920',\n",
    "#   'speechiness': 0.0936,\n",
    "#   'tempo': 149.976,\n",
    "#   'valence': 0.634,\n",
    "#   'year': 1920}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-formation",
   "metadata": {},
   "source": [
    "> So it looks like the earliest track is indeed from 1990."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-gather",
   "metadata": {},
   "source": [
    "Now let's find the year of the most recent track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "neither-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTracksDesc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "miniature-tribe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acousticness': 0.778,\n",
       "  'artists': \"['not applicable', 'Riccardo Muti', 'Wiener Philharmoniker']\",\n",
       "  'danceability': 0.711,\n",
       "  'duration_ms': 217360,\n",
       "  'energy': 0.0983,\n",
       "  'explicit': 0,\n",
       "  'id': '55VqxXi21UxYikKbrMXv54',\n",
       "  'instrumentalness': 0.0,\n",
       "  'key': 1,\n",
       "  'liveness': 0.62,\n",
       "  'loudness': -28.235,\n",
       "  'mode': 1,\n",
       "  'name': \"Neujahrsgruß / New Year's Address / Allocution du Nouvel An\",\n",
       "  'popularity': 29,\n",
       "  'release_date': '2021-01-08',\n",
       "  'speechiness': 0.899,\n",
       "  'tempo': 111.518,\n",
       "  'valence': 0.359,\n",
       "  'year': 2021}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedTracksDesc.take(1)\n",
    "\n",
    "\n",
    "# [{'acousticness': 0.778,\n",
    "#   'artists': \"['not applicable', 'Riccardo Muti', 'Wiener Philharmoniker']\",\n",
    "#   'danceability': 0.711,\n",
    "#   'duration_ms': 217360,\n",
    "#   'energy': 0.0983,\n",
    "#   'explicit': 0,\n",
    "#   'id': '55VqxXi21UxYikKbrMXv54',\n",
    "#   'instrumentalness': 0.0,\n",
    "#   'key': 1,\n",
    "#   'liveness': 0.62,\n",
    "#   'loudness': -28.235,\n",
    "#   'mode': 1,\n",
    "#   'name': \"Neujahrsgruß / New Year's Address / Allocution du Nouvel An\",\n",
    "#   'popularity': 29,\n",
    "#   'release_date': '2021-01-08',\n",
    "#   'speechiness': 0.899,\n",
    "#   'tempo': 111.518,\n",
    "#   'valence': 0.359,\n",
    "#   'year': 2021}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-competition",
   "metadata": {},
   "source": [
    "> So we can see that the most recent track is from 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-gauge",
   "metadata": {},
   "source": [
    "Now if we look at the previous two spark jobs, which involved sorting, we can see that the jobs led to shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-expansion",
   "metadata": {},
   "source": [
    "<img src=\"./shuffle_events.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-reader",
   "metadata": {},
   "source": [
    "> Now this makes sense because if we think about sorting, it involves grouping together our data, and this involves moving our data from across partitions and nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-techno",
   "metadata": {},
   "source": [
    "One way, perhaps to limit the amount of shuffling is to limit the amount of data that needs to be transferred.  For example above, we are moving across, and returning the entire record.  But what we really care is about finding the minimum and maximum years in our dataset, not the movies associated with them.  So let's write a new function that only returns the maximum year in the dataset.  \n",
    "\n",
    "Hopefully, this will reduce shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capable-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedYearsDesc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "robust-cream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2021]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedYearsDesc.take(1)\n",
    "\n",
    "# [2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-retirement",
   "metadata": {},
   "source": [
    "### Aggregate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-columbus",
   "metadata": {},
   "source": [
    "Let's wrap up by performing a couple of aggregate metrics.  Let's begin by simply counting the number of records in each year.  To do this we'll need a group by.  \n",
    "\n",
    "> Remember that this query results in shuffling, so try to reduce the amount of shuffling by limiting the amount of data that needs to be transferred for this query.\n",
    "\n",
    "> Also, use Pyspark to sort the return values by year, as seen in the answer below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "concrete-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_by_year = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "packed-machine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1920, 349), (1921, 156), (1922, 121), (1923, 185), (1924, 236)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_by_year[:5]\n",
    "\n",
    "# [(1920, 349), (1921, 156), (1922, 121), (1923, 185), (1924, 236)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "variable-protection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2017, 2156), (2018, 2714), (2019, 2329), (2020, 4294), (2021, 1840)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_by_year[-5:]\n",
    "\n",
    "# [(2017, 2156), (2018, 2714), (2019, 2329), (2020, 4294), (2021, 1840)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-definition",
   "metadata": {},
   "source": [
    "Now from here, we could use our RDDs to calculate some aggregate metrics, like the average tempo, or loudness per year.  However, doing so purely in RDDs is pretty tricky.  If you'd like to give it it a shot, take a look at [this link](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala) to see how you might do so.  \n",
    "\n",
    "Operations like this, will become much easier when move over to using dataframes in Pyspark, which we'll move to in the next section.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-recipient",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-snapshot",
   "metadata": {},
   "source": [
    "[Reduce by Key Stackoverflow](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
