{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automated-groove",
   "metadata": {},
   "source": [
    "# Spark RDDs Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-agency",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-strand",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice working with Spark RDDs and the Spark UI.  Along the way, we'll explore the different attributes about RDDs:\n",
    "    \n",
    "* in memory storage\n",
    "* distributed jobs performed in parallel \n",
    "* resiliency through the use of dags\n",
    "* and lazy operations that only kick off when an action is invoked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-modem",
   "metadata": {},
   "source": [
    "Ok, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-shooting",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-limitation",
   "metadata": {},
   "source": [
    "Let's begin by creating our spark context.  Set the number of partitions equal to 2, and pass through the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prompt-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "activated-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"s3://jigsaw-labs/spotify_tracks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entire-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-department",
   "metadata": {},
   "source": [
    "So if we look at one of the records, we can see that we have a list of song tracks with the artist name, a track id, and various attributes of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acceptable-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acousticness        float64\n",
       "artists              object\n",
       "danceability        float64\n",
       "duration_ms           int64\n",
       "energy              float64\n",
       "explicit              int64\n",
       "id                   object\n",
       "instrumentalness    float64\n",
       "key                   int64\n",
       "liveness            float64\n",
       "loudness            float64\n",
       "mode                  int64\n",
       "name                 object\n",
       "popularity            int64\n",
       "release_date         object\n",
       "speechiness         float64\n",
       "tempo               float64\n",
       "valence             float64\n",
       "year                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-vocabulary",
   "metadata": {},
   "source": [
    "Ok, now let's feed this list of tracks into a Spark cluster.  We'll first we'll need to create our spark cluster.  Set the application name to `musicTracks` and connect locally, allocating 2 cores to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "golden-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"musicTracks\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naval-spending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'musicTracks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName\n",
    "# 'music_tracks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operating-orbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[2]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master\n",
    "\n",
    "# 'local[2]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-right",
   "metadata": {},
   "source": [
    "Now that the cluster is setup, we might as well open up the Spark UI now so that it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "miniature-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>musicTracks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=musicTracks>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-internet",
   "metadata": {},
   "source": [
    "Ok, next let's create an RDD from our music tracks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abstract-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuing-allowance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_rdd = sc.parallelize(tracks)\n",
    "\n",
    "tracks_rdd\n",
    "# ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-orchestra",
   "metadata": {},
   "source": [
    "Now if we look at the Spark UI, note that there are no jobs listed, and the event timeline should be blank.  So this goes back to Spark being lazy.  Even though we directed Spark to read in the data, Spark will not take action until we invoke an action.  \n",
    "\n",
    "> In fact, if we look at the executors page of the dashboard, we'll see that none of our memory was consumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-oracle",
   "metadata": {},
   "source": [
    "<img src=\"./executors.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-plastic",
   "metadata": {},
   "source": [
    "Ok, so now let's call our first action by asking for the number of records in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pregnant-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174389"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "announced-walnut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acousticness': 0.991,\n",
       "  'artists': \"['Mamie Smith']\",\n",
       "  'danceability': 0.598,\n",
       "  'duration_ms': 168333,\n",
       "  'energy': 0.2239999999999999,\n",
       "  'explicit': 0,\n",
       "  'id': '0cS0A1fUEUd1EW3FcF8AEI',\n",
       "  'instrumentalness': 0.000522,\n",
       "  'key': 5,\n",
       "  'liveness': 0.379,\n",
       "  'loudness': -12.628,\n",
       "  'mode': 0,\n",
       "  'name': 'Keep A Song In Your Soul',\n",
       "  'popularity': 12,\n",
       "  'release_date': '1920',\n",
       "  'speechiness': 0.0936,\n",
       "  'tempo': 149.976,\n",
       "  'valence': 0.634,\n",
       "  'year': 1920}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-electronics",
   "metadata": {},
   "source": [
    "### Querying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-origin",
   "metadata": {},
   "source": [
    "From here, we can perform some queries on the data.  Let's begin by getting a sense of the range of our data.  Sort the tracks by year and then let's look at the first record.\n",
    "\n",
    "> Take a look at the [Pyspark documentation](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#sortBy) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fiscal-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTracks = tracks_rdd.sortBy(lambda track: track['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "assumed-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acousticness': 0.991,\n",
       "  'artists': \"['Mamie Smith']\",\n",
       "  'danceability': 0.598,\n",
       "  'duration_ms': 168333,\n",
       "  'energy': 0.2239999999999999,\n",
       "  'explicit': 0,\n",
       "  'id': '0cS0A1fUEUd1EW3FcF8AEI',\n",
       "  'instrumentalness': 0.000522,\n",
       "  'key': 5,\n",
       "  'liveness': 0.379,\n",
       "  'loudness': -12.628,\n",
       "  'mode': 0,\n",
       "  'name': 'Keep A Song In Your Soul',\n",
       "  'popularity': 12,\n",
       "  'release_date': '1920',\n",
       "  'speechiness': 0.0936,\n",
       "  'tempo': 149.976,\n",
       "  'valence': 0.634,\n",
       "  'year': 1920}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedTracks.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-romance",
   "metadata": {},
   "source": [
    "> So it looks like the earliest track is indeed from 1990."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-billy",
   "metadata": {},
   "source": [
    "Now let's find the year of the most recent track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "printable-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTracksDesc = tracks_rdd.sortBy(lambda track: track['year'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-journalist",
   "metadata": {},
   "source": [
    "> So we can see that the most recent track is from 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "recovered-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acousticness': 0.778,\n",
       "  'artists': \"['not applicable', 'Riccardo Muti', 'Wiener Philharmoniker']\",\n",
       "  'danceability': 0.711,\n",
       "  'duration_ms': 217360,\n",
       "  'energy': 0.0983,\n",
       "  'explicit': 0,\n",
       "  'id': '55VqxXi21UxYikKbrMXv54',\n",
       "  'instrumentalness': 0.0,\n",
       "  'key': 1,\n",
       "  'liveness': 0.62,\n",
       "  'loudness': -28.235,\n",
       "  'mode': 1,\n",
       "  'name': \"Neujahrsgru√ü / New Year's Address / Allocution du Nouvel An\",\n",
       "  'popularity': 29,\n",
       "  'release_date': '2021-01-08',\n",
       "  'speechiness': 0.899,\n",
       "  'tempo': 111.518,\n",
       "  'valence': 0.359,\n",
       "  'year': 2021}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedTracksDesc.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-silver",
   "metadata": {},
   "source": [
    "Now if we look at the previous two spark jobs, which involved sorting, we can see that the jobs led to shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-gateway",
   "metadata": {},
   "source": [
    "<img src=\"./shuffle_events.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-found",
   "metadata": {},
   "source": [
    "> Now this makes sense because if we think about sorting, it involves grouping together our data, and this involves moving our data from across partitions and nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-diagram",
   "metadata": {},
   "source": [
    "One way, perhaps to limit the amount of shuffling is to limit the amount of data that needs to be transferred.  For example above, we are moving across, and returning the entire record.  But what we really care is about finding the minimum and maximum years in our dataset, not the movies associated with them.  So let's write a new function that only returns the maximum year in the dataset.  \n",
    "\n",
    "Hopefully, this will reduce shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coastal-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedYearsDesc = tracks_rdd.map(lambda track: track['year']).sortBy(lambda year: year, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "periodic-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2021]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedYearsDesc.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-baptist",
   "metadata": {},
   "source": [
    "### Aggregate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-watershed",
   "metadata": {},
   "source": [
    "Let's wrap up by performing a couple of aggregate metrics.  Let's begin by simply counting the number of records in each year.  To do this we'll need a group by.  \n",
    "\n",
    "> Remember that this query results in shuffling, so try to reduce the amount of shuffling by limiting the amount of data that needs to be transferred for this query.\n",
    "\n",
    "> Also, use Pyspark to sort the return values by year, as seen in the answer below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "convertible-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_by_year = tracks_rdd.map(lambda track: track['year']). \\\n",
    "groupBy(lambda year: year).sortBy(lambda year_amount: year_amount[0]).map(lambda year_amount: (year_amount[0], len(year_amount[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "infinite-mauritius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1920, 349), (1921, 156), (1922, 121), (1923, 185), (1924, 236)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_by_year[:5]\n",
    "\n",
    "# [(1920, 349), (1921, 156), (1922, 121), (1923, 185), (1924, 236)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "false-cologne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2017, 2156), (2018, 2714), (2019, 2329), (2020, 4294), (2021, 1840)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_by_year[-5:]\n",
    "\n",
    "# [(2017, 2156), (2018, 2714), (2019, 2329), (2020, 4294), (2021, 1840)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-stake",
   "metadata": {},
   "source": [
    "Now from here, we could use our RDDs to calculate some aggregate metrics, like the average tempo, or loudness per year.  However, doing so purely in RDDs is pretty tricky.  If you'd like to give it it a shot, take a look at [this link](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala) to see how you might do so.  \n",
    "\n",
    "Operations like this, will become much easier when move over to using dataframes in Pyspark, which we'll move to in the next section.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-rochester",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-mirror",
   "metadata": {},
   "source": [
    "[Reduce by Key Stackoverflow](https://stackoverflow.com/questions/40087483/spark-average-of-values-instead-of-sum-in-reducebykey-using-scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
